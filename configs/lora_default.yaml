# ─────────────────────── LoRA fine-tuning defaults ───────────────────────────
# Modelo base y task
model_name: facebook/nllb-200-distilled-600M   # traductor 600 M params
src_lang: eng_Latn
tgt_lang: tgl_Latn                             # Tagalo

# Adaptadores LoRA
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj","k_proj","v_proj","o_proj",
                   "gate_proj","down_proj","up_proj"]
  bias: "none"

# Entrenamiento
training:
  max_steps: 5000
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  warmup_steps: 250
  logging_steps: 50
  eval_steps: 500
  save_steps: 500
  fp16: true
  seed: 42

# Rutas (se resuelven vía paths.yaml)
paths: ${paths}
