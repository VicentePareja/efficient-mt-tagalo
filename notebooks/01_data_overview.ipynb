{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 ‚Äì Data overview  \n",
    "\n",
    "üö¶ *Smoke test*: tama√±o de corpus y distribuci√≥n de splits*.\n",
    "\n",
    "> Ejecuta este notebook **despu√©s** de haber corrido `src/data/download_flores.py` y\n",
    "> procesado los archivos en `data/processed/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "DATA_DIR = Path('../data/processed')  # ajusta si cambias la ruta\n",
    "\n",
    "# Enumerar archivos txt ‚Üí train / dev / test\n",
    "files = sorted(DATA_DIR.glob('*.txt'))\n",
    "print('Encontrados:', [f.name for f in files])\n",
    "\n",
    "sizes = {f.stem: sum(1 for _ in f.open()) for f in files}\n",
    "display(pd.DataFrame({'sentences': sizes}).T)\n",
    "\n",
    "# Distribuci√≥n simple de longitudes (tokens) para un split\n",
    "sample_file = DATA_DIR / 'train.tgl.txt'\n",
    "if sample_file.exists():\n",
    "    lengths = [len(line.split()) for line in sample_file.open()]\n",
    "    print('Longitud media (tokens) en train:', sum(lengths) / len(lengths))\n",
    "    print('Histograma r√°pido:', Counter(map(lambda x: min(x//5*5, 40), lengths)))\n",
    "else:\n",
    "    print('‚ö†Ô∏è  A√∫n no existe train.tgl.txt ‚Äì ejecuta el pipeline de datos.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
